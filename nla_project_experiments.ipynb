{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nla_project_experiments.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPFGuyoF9ui95L2DgLpZNqE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sireesha-vakada/scisumm-corpus/blob/master/nla_project_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8NZNo0JC0qE",
        "colab_type": "code",
        "outputId": "e72d6caa-5fc6-47a3-ce71-2840bb5f2612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import csv\n",
        "import xml.etree.ElementTree as ET\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.util import ngrams\n",
        "import re \n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import pandas as pd \n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK0znzHEpeJZ",
        "colab_type": "text"
      },
      "source": [
        "**PRE**-**PROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-nAF2I62S4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parseXML(xmlfile):\n",
        "  tree = ET.parse(xmlfile)\n",
        "  root = tree.getroot()\n",
        "  heads = []\n",
        "  # print (\"hi\")\n",
        "  # print (root.tag)\n",
        "  ref_sent = []\n",
        "  for child in root:\n",
        "    # print (child.tag, child.attrib)\n",
        "    for each in child:\n",
        "      ref_sent.append(each.text)\n",
        "  \n",
        "  return ref_sent\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t46l-snnKiux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_string(s):\n",
        "  s = s.split(\">\")\n",
        "  t = []\n",
        "  # print (\"s = \", s)\n",
        "  i = 0\n",
        "  for each in s:\n",
        "    if(i%2 != 0):\n",
        "      temp = str(each)\n",
        "      temp = temp.split(\"<\")[0]\n",
        "      # print (temp)\n",
        "      t.append(temp)\n",
        "    i = i+1\n",
        "  \n",
        "  return t\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRckqZfqB8j8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_citations(file):\n",
        "  file_read = open(file, \"r\")\n",
        "  citations = file_read.readlines()\n",
        "  total_cit = []\n",
        "  for each in citations:\n",
        "    if each != \"\\n\":\n",
        "      # print (each)\n",
        "      tokens = each.split(\"|\")\n",
        "      ref_article = each.partition(\"Reference Article:\")[2]\n",
        "      ref_article = ref_article.partition(\"|\")[0]\n",
        "\n",
        "      cit_article = each.partition(\"Citing Article:\")[2]\n",
        "      cit_article = cit_article.partition(\"|\")[0]\n",
        "      \n",
        "      cit_text = each.partition(\"Citation Text:\")[2]\n",
        "      cit_text = cit_text.partition(\"|\")[0]\n",
        "\n",
        "      ref_text = each.partition(\"Reference Text:\")[2]\n",
        "      ref_text = ref_text.partition(\"|\")[0]\n",
        "\n",
        "      ref_texts = extract_string(ref_text)\n",
        "      cit_texts = extract_string(cit_text) \n",
        "\n",
        "      # print(\"cit_text = \", len(cit_texts))\n",
        "      string = \"\"\n",
        "      if(len(cit_texts) > 1):\n",
        "        for i in range(len(cit_texts)):\n",
        "          if i > 0:\n",
        "            string = string[:-1] + \" and \" + cit_texts[i]\n",
        "          else:\n",
        "            string = cit_texts[i]\n",
        "        print (string)\n",
        "      else:\n",
        "        string = str(cit_texts[0])\n",
        "      \n",
        "      total_cit.append(string)\n",
        "    \n",
        "  return total_cit\n",
        "      \n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yia_iyYPH0pe",
        "colab_type": "text"
      },
      "source": [
        "**DATA EXTRACTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfcRl1n42KMy",
        "colab_type": "code",
        "outputId": "20f4c710-15de-429b-93fb-2ff16f702892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "\n",
        "ref_sentences = parseXML(\"C00-2123.xml\")\n",
        "cit_sentences = get_citations(\"C00-2123.txt\")\n",
        "\n",
        "print (len(ref_sentences))\n",
        "print (len(cit_sentences))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al and (1996), Och et al and (2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.\n",
            "This article will present a DP-based beam search decoder for the IBM4 translation model and A preliminary version of the work presented here was published in Tillmann and Ney (2000).\n",
            "It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w and , wm−1, um\n",
            "203\n",
            "18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8UDqbgfpnSd",
        "colab_type": "text"
      },
      "source": [
        "**TF_IDF VECTORS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd4gc4rz4AB0",
        "colab_type": "code",
        "outputId": "d4d13355-102b-4696-b47b-99431b5b09ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "cit = cit_sentences[0]\n",
        "ref = ref_sentences[0]\n",
        "len_ref = len(ref_sentences)\n",
        "len_cit = len(cit_sentences)\n",
        "all_sent = []\n",
        "\n",
        "for each in ref_sentences:\n",
        "  all_sent.append(each)\n",
        "\n",
        "for each in cit_sentences:\n",
        "  all_sent.append(each)\n",
        "\n",
        "n_gram = 1;\n",
        "vectorizer = TfidfVectorizer(ngram_range = (n_gram,n_gram))\n",
        "tf_df = vectorizer.fit_transform(all_sent) \n",
        "X = tf_df.toarray()\n",
        "print (len(vectorizer.get_feature_names()))\n",
        "print (X.shape)\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "997\n",
            "(221, 997)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdaH7cHIlney",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy import spatial\n",
        "\n",
        "def cosine(a,b):\n",
        "  ans = 1 - spatial.distance.cosine(a,b)\n",
        "  return ans\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJGA0mJBWq9-",
        "colab_type": "code",
        "outputId": "e0c2b706-a9dd-43e6-de47-7384ff0c960d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "#cosine similarity\n",
        "import numpy\n",
        "\n",
        "\n",
        "def find_sent(features, k):\n",
        "  \n",
        "  for i in range(len_cit):\n",
        "    maxi = 0\n",
        "    index = 0\n",
        "    value = []\n",
        "    indices = []\n",
        "    for j in range(len_ref):\n",
        "      a = features[j]\n",
        "      b = features[i + len_ref]\n",
        "      # print(i+len_ref, j)\n",
        "      val = cosine(a,b)\n",
        "      if(val  > 0):\n",
        "        value.append(val)\n",
        "        indices.append(j)\n",
        "\n",
        "      if(val > maxi):\n",
        "        maxi = val\n",
        "        index = j\n",
        "    # print (\"max_val = \", maxi)\n",
        "    # print (\"cit_sent = \", cit_sentences[i])\n",
        "    # print (\"ref_sent = \", ref_sentences[index])\n",
        "    s = numpy.array(value)\n",
        "    t = numpy.argsort(s)\n",
        "\n",
        "    if(i == 2):\n",
        "      # print (value)\n",
        "      # print (t)\n",
        "      # print (indices)\n",
        "      print (\"cit_sent = \", cit_sentences[i])\n",
        "      if (k > len(t)):\n",
        "        k1 = len(t)\n",
        "      else:\n",
        "        k1 = k\n",
        "\n",
        "      for each in range(k1):\n",
        "        ind = indices[t[-(each+1)]]\n",
        "        # print (ind)\n",
        "        print (value[t[-(each+1)]])\n",
        "        print (\"ref_text = \", ref_sentences[ind])\n",
        "        \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "# tf-idf results\n",
        "find_sent(X, 5)\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cit_sent =  The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.\n",
            "0.30608072060668\n",
            "ref_text =  The details are given in (Och and Ney, 2000).\n",
            "0.27622423694666665\n",
            "ref_text =  We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.\n",
            "0.25758932627232634\n",
            "ref_text =  We use a solution to this problem similar to the one presented in (Och et al., 1999), where target words are joined during training.\n",
            "0.25071769987692694\n",
            "ref_text =  This approach is compared to another reordering scheme presented in (Berger et al., 1996).\n",
            "0.24962068222017642\n",
            "ref_text =  In this paper, we have presented a new, eÆcient DP-based search procedure for statistical machine translation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiSStjD6lzfy",
        "colab_type": "code",
        "outputId": "03eb5d57-db94-47a5-99af-29b412831de4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "#word2vec model\n",
        "\n",
        "import gensim \n",
        "from gensim.models import Word2Vec \n",
        "\n",
        "word2vec_voc = []\n",
        "for i in ref_sentences:\n",
        "  temp = []\n",
        "  for j in word_tokenize(i):\n",
        "    temp.append(j.lower())\n",
        "  word2vec_voc.append(temp)\n",
        "\n",
        "for i in cit_sentences:\n",
        "  temp = []\n",
        "  for j in word_tokenize(i):\n",
        "    temp.append(j.lower())\n",
        "  word2vec_voc.append(temp)\n",
        "\n",
        "\n",
        "CBOW_model = gensim.models.Word2Vec(word2vec_voc, min_count =1,  size=100,  window=5)\n",
        "\n",
        "print(CBOW_model.similarity('researchers', 'language'))\n",
        "print(CBOW_model.corpus_total_words)\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.44042498\n",
            "6070\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COur64R4JjJw",
        "colab_type": "code",
        "outputId": "13e787a4-b0fa-454f-90e8-d34d429d82c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "print(len(word2vec_voc))\n",
        "# print(CBOW_model.accuracy)\n",
        "data = CBOW_model[\"language\"]\n",
        "print (data.shape)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "221\n",
            "(100,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-9DCBOQMM4y",
        "colab_type": "code",
        "outputId": "298d4ea0-d0de-48ba-fb66-0c5951291d09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# all_sent - consists of all the sentences\n",
        "import numpy as np\n",
        "\n",
        "total_voc = []\n",
        "\n",
        "for sent in all_sent:\n",
        "  for word in word_tokenize(sent):\n",
        "    total_voc.append(word.lower())\n",
        "\n",
        "print (len(total_voc))\n",
        "tf_awv_feature = []\n",
        "\n",
        "#only based on tf and with wor2vec weighted average similarities\n",
        "\n",
        "for sent in all_sent:\n",
        "  temp = np.zeros(100)\n",
        "  for word in word_tokenize(sent):\n",
        "    word = word.lower()\n",
        "    temp = temp + CBOW_model[word] \n",
        "  temp = temp/len(word_tokenize(sent))\n",
        "  tf_awv_feature.append(temp)\n",
        "\n",
        "print(len(tf_awv_feature))\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6070\n",
            "221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya09caogU7x1",
        "colab_type": "code",
        "outputId": "944f54b7-10a1-4fd9-88d4-a73cdb43a078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "source": [
        "#another cosine similarity with word2vec\n",
        "\n",
        "find_sent(tf_awv_feature,  15)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cit_sent =  The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.\n",
            "0.9996862113736001\n",
            "ref_text =  To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).\n",
            "0.9996860280039077\n",
            "ref_text =  The details are given in (Och and Ney, 2000).\n",
            "0.9996379851935089\n",
            "ref_text =  We use a solution to this problem similar to the one presented in (Och et al., 1999), where target words are joined during training.\n",
            "0.9996348379269554\n",
            "ref_text =  For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j), only the score for the best path reaching j has to be stored.\n",
            "0.9996292855136879\n",
            "ref_text =  The computing time, the number of search errors, and the multi-reference WER (mWER) are shown as a function of t0.\n",
            "0.9996174117526234\n",
            "ref_text =  The proof is given in (Tillmann, 2000).\n",
            "0.9996174117526234\n",
            "ref_text =  The proof is given in (Tillmann, 2000).\n",
            "0.9996093879186284\n",
            "ref_text =  We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.\n",
            "0.9995910182699748\n",
            "ref_text =  e0; e are the last two target words, C is a coverage set for the already covered source positions and j is the last position visited.\n",
            "0.9995851823472108\n",
            "ref_text =  The details are given in (Tillmann, 2000).\n",
            "0.9995737309693125\n",
            "ref_text =  A procedural definition to restrict1In the approach described in (Berger et al., 1996), a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search.\n",
            "0.9995574109599502\n",
            "ref_text =  To be short, we omit the target words e; e0 in the formulation of the search hypotheses.\n",
            "0.9995559402408131\n",
            "ref_text =  Ignoring the identity of the target language words e and e0, the possible partial hypothesis extensions due to the IBM restrictions are shown in Table 2.\n",
            "0.9995554317439839\n",
            "ref_text =  However, dynamic programming can be used to find the shortest tour in exponential time, namely in O(n22n), using the algorithm by Held and Karp.\n",
            "0.9995519685760771\n",
            "ref_text =  Each distance in the traveling salesman problem now corresponds to the negative logarithm of the product of the translation, alignment and language model probabilities.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh0MDUnUebOm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b3628e2-8bbf-4637-94e4-ab795c15c8dd"
      },
      "source": [
        "f = [1,2,3,4,5]\n",
        "print (f[-1])"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k7OQKgMZKNv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14bccb1f-0663-4c4f-a3c8-ef83009e4e1e"
      },
      "source": [
        "print (f[-2])"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz5rFZqkZMjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}